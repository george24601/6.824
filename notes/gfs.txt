primary is partitioned from backup B
client appends 1
primary sends 1 to itself and backup A
reports failure to client
meanwhile client 2 may backup B and observe old value

application observe behaviors that are non-observable in an ideal system
    e.g., reading stale data
    e.g., duplicate append records

an ordinary Linux file for each chunk

master server knows directory hierarchy
    for dir, what files are in it
    for file, knows chunk servers for each 64 MB
    master keeps state in memory
      64 bytes of metadata per each chunk
    master has private recoverable database for metadata
      master can recovery quickly from power failure
    shadow masters that lag a little behind master
      can be promoted to master

client write:
    ask master where to store
    maybe master chooses a new set of chunk servers if crossing 64 MB
    one chunk server is primary
    it chooses order of updates and forwards to two backups


Master fault tolerance
  Stores limited information persistently
    name spaces (directories)
    file-to-chunk mappings

log changes to these two in a log
    log is replicated on several backups
    clients operations that modify state return *after* recording changes in *logs*

Limiting the size of the log
    Make a checkpoint of the master state
    Remove all operations from log from before checkpoint
    Checkpoint is replicated to backups

chunk location information is recreated by asking chunk servers

Chunk fault tolerance
  Master grants a chunk lease to one of the replicas
    That replica is the primary chunk server
  Primary determines orders operations
  Clients pushes data to replicas
    Replicas form a chain
    Chain respects network topology
    Allows fast replication
  Client sends write request to primary
    Primary assigns sequence number
    Primary applies change locally
    Primary forwards request to replicates
    Primary responds to client after receiving acks from all replicas
  If one replica doesn't respond, client retries
  Master replicates chunks if number replicas drop below some number
  Master rebalances replicas

Consistency of chunks
  Some chunks may get out of date
    they miss mutations
  Detect stale data with chunk version number
    before handing out a lease
      increments chunk version number
       sends it to primary and backup chunk servers
    master and chunk servers store version persistently
  Send version number also to client
  Version number allows master and client to detect stale replicas

Concurrent writes/appends
  clients may write to the same region of file concurrently
  the result is some mix of those writes--no guarantees
    few applications do this anyway, so it is fine
    concurrent writes on Unix can also result in a strange outcome
  many client may want to append concurrently to, e.g., a log file
    GFS support atomic, at-least-once append
    the primary chunk server chooses the offset where to append a record
    sends it to all replicas.
    if it fails to contact a replica, the primary reports an error to client
    client retries; if retry succeeds:
      some replicas will have the append twice (the ones that succeeded)
    the file may have a "hole" too
      when GFS pads to chunk boundary, if an append would across chunk boundary

Consistency model
  Strong consistency for directory operations
    Master performs changes to metadata atomically
    Directory operations follow the "ideal"
    But, when master is off-line, only shadow masters
      Read-only operations only, which may return stale data
  Weak consistency for chunk operations
    A failed mutation leaves chunks inconsistent
      The primary chunk server updated chunk
      But then failed and the replicas are out of date
    A client may read an not-up-to-date chunk
    When client refreshes lease it will learn about new version #
  Authors claims weak consistency is not a big problems for apps    
    Most file updates are append-only updates
      Application can use UID in append records to detect duplicates
      Application may just read less data (but not stale data)
    Application can use temporary files and atomic rename

  what less well in GFS?
    fault-tolerance of master
    small files (master a bottleneck)
    concurrent updates to same file from many clients (except appends)
